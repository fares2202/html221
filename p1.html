<!DOCTYPE html>
<html>
<head>
	<title>computer architecture
</title>
</head>
<body>
	<h1>computer architecture (Data-Level Parallelism in Vector, SIMD, and GPU Architectures)
</h1>
	<h2>links</h2>
	<ul>
  <li><a href="index.html">main page</a></li>
  <li><a href="p1.html">introduction of Data-Level Parallelism in Vector, SIMD, and GPU Architectures </a></li>
  <li><a href="p2.html">Data-Level Parallelism in Vector, SIMD, and GPU Architectures table</a></li>
   <li><a href="p3.html">Data-Level Parallelism in Vector, SIMD, and GPU Architectures image</a></li>
   <li><a href="p4.html">Data-Level Parallelism in Vector, SIMD, and GPU Architectures (Historical Perspective and References)</a></li>
</ul>
 <p  >
  <h3>introduction </h3>
 question for the single instruction, multiple data (SIMD) architecture, which
Chapter 1 introduced, has always been just how wide a set of applications has
significant data-level parallelism (DLP). Fifty years later, the answer is not only
the matrix-oriented computations of scientific computing, but also the media oriented image and sound processing. Moreover, since a single instruction can
launch many data operations, SIMD is potentially more energy efficient than
multiple instruction multiple data (MIMD), which needs to fetch and execute
one instruction per data operation. These two answers make SIMD attractive for
Personal Mobile Devices. Finally, perhaps the biggest advantage of SIMD versus MIMD is that the programmer continues to think
 </p>

</body>
</html>